{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j1_GBEAaf2e0"
   },
   "source": [
    "# Projet de fin de semestre : SPAM CLASSIFIEUR\n",
    "\n",
    "## Réalisé par :\n",
    "\n",
    "- TAIBI      Abdessetar 191932027573 GROUPE 1\n",
    "\n",
    "- DJENANE    Nihad      191931040689 GROUPE 1\n",
    "\n",
    "- TALABOULMA Roumaissa  191932021195 GROUPE 1\n",
    "\n",
    "- M'BAREK    Lydia      181831064011 GROUPE 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L'objectif de ce projet est de développer un système de détection de spam\n",
    "\n",
    "Dans ce projet, nous avons utilise un ensemble d'e-mails étiquetés comme étant du spam \n",
    "et nous avons utilise différentes approches pour développer notre classifieur de détection de spam. \n",
    "\n",
    "Les performances de chaque approche seront comparées pour évaluer leur efficacité et déterminer la meilleure approche pour la détection de spam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xqXVyDtlf2e4"
   },
   "source": [
    "# Importation des librairies necessaires au travail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons pour dans ce projet les librairies suivantes :\n",
    "\n",
    "- **NLTK** : Librairie de traitement de langague, contient des méthodes de traitement de text (stemming, tokenization, ...). Nous utilisons *SnowballStemmer* pour réduire les tokens en à leurs radicaux.\n",
    "\n",
    "- **sklearn** : Nous utilisons cette librairie pour entrainer des algorithmes d'apprentissage machine comme les SVM et Reseaux de neurones Nous utilisons les metriques proposées par cette librairie pour comparer les résultats obtenus par chaque modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Z7Ut1MyHf2e4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ekH1R_JWf2e5"
   },
   "source": [
    "# Étape 1 : préparation des données (Preprocessing)\n",
    "Avant de développer le classifieur de détection de spam, une préparation des données sera effectuée, comprenant la collecte des e-mails, leur nettoyage et leur transformation en un format utilisable pour l'entraînement et la validation des modèles de classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalisation des emails : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GsEfLfb6f2e6",
    "outputId": "6bfb8c10-c80a-43ad-cd7f-7bfcd73e64e6"
   },
   "outputs": [],
   "source": [
    "def clean_mail(mail):\n",
    "    # Convertir en minuscules\n",
    "    mail = mail.lower()\n",
    "    # Supprimer les en-têtes et les pieds de page de l'e-mail\n",
    "    mail = re.sub(r'^.*?(\\n\\n|\\r\\n\\r\\n)', '', mail, flags=re.S)\n",
    "    # supprimer les espaces et saut de ligne\n",
    "    mail = re.sub(r'\\s+', ' ', mail)\n",
    "    # Supprimer les balises HTML\n",
    "    mail = re.sub(r'<.*?>', ' ', mail)\n",
    "    # Normaliser les URL en remplaçant par \"httpaddr\"\n",
    "    mail  = re.sub(r'(http|https)://[^\\s]+', \"httpaddr\", mail)\n",
    "    # Normaliser les adresses e-mail en remplaçant par \"emailaddr\"\n",
    "    mail  = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b', \"emailaddr\", mail)\n",
    "    # Remplacer le symbole dollar ($) par \"dollar\"\n",
    "    mail = re.sub('\\$', 'dollar', mail)\n",
    "    # Remplacer tous les nombres par \"number\"\n",
    "    mail = re.sub('\\d+', 'number', mail)\n",
    "    # Séparer les mots de l'e-mail en une liste\n",
    "    mail_words = mail.split()\n",
    "    # Initialiser le stemmer et le stopwords pour normaliser les mots\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Normaliser chaque mot de l'e-mail en sa forme radicale \n",
    "    mail_words = [stemmer.stem(word) for word in mail_words]\n",
    "    # Joindre les mots normalisés pour reformer l'e-mail\n",
    "    mail_cleaned = \" \".join(mail_words)\n",
    "    # Supprimer les caractères spéciaux et les espaces supplémentaires\n",
    "    mail_cleaned = re.sub(r'[^\\w\\s]|_', ' ', mail_cleaned)\n",
    "    mail_cleaned = re.sub(r'\\s+', ' ', mail_cleaned).strip()\n",
    "    # Renvoyer l'e-mail nettoyé\n",
    "    return mail_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    # Création d'une liste vide pour stocker les données\n",
    "    data = []\n",
    "    # Récupération des noms de tous les fichiers présents dans le chemin spécifié\n",
    "    files = os.listdir(path)    \n",
    "    # Parcours de tous les fichiers présents dans le répertoire\n",
    "    for file in files:\n",
    "        if file == \"cmds\":\n",
    "            pass\n",
    "        # Ouverture du fichier en mode lecture \n",
    "        processed_file = open(path +\"/\"+ file, encoding=\"ISO-8859-1\")        \n",
    "        # Lecture du contenu du fichier\n",
    "        words_list = processed_file.read()        \n",
    "        # Ajout du contenu du fichier à la liste de données\n",
    "        data.append(words_list)        \n",
    "        # Fermeture du fichier\n",
    "        processed_file.close()    \n",
    "    # Renvoi de la liste de données\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mails_path = 'Data/spam_2/spam_2'\n",
    "hard_ham_mails_path = 'Data/hard_ham/hard_ham'\n",
    "easy_ham_mails_path = 'Data/easy_ham/easy_ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donnes\n",
    "data_spam_mails = get_data(spam_mails_path)\n",
    "data_hard_ham_mails = get_data(hard_ham_mails_path)\n",
    "data_easy_ham_mails = get_data(easy_ham_mails_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_mails(data_mail):\n",
    "    # Création d'une liste vide pour stocker les mails nettoyés\n",
    "    mails_clean = []\n",
    "    # Parcours de la liste de mails \n",
    "    for mail in data_mail:\n",
    "        # nettoyer le mail et l'ajouter à la liste \"mails_clean\"\n",
    "        mails_clean.append(clean_mail(mail))\n",
    "    return mails_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n"
     ]
    }
   ],
   "source": [
    "# nettoyer les emails spam\n",
    "spam_mails_clean = clean_all_mails(data_spam_mails)\n",
    "print(len(spam_mails_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyer les emails ham\n",
    "hard_ham_mails_clean = clean_all_mails(data_hard_ham_mails)\n",
    "easy_ham_mails_clean = clean_all_mails(data_easy_ham_mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                       0\n",
       "0     greetings you are receiv this letter becaus yo...\n",
       "1     the need for safeti is real in number you migh...\n",
       "2     bonus fat absorb as seen on tv includ free wit...\n",
       "3     bonus fat absorb as seen on tv includ free wit...\n",
       "4     govern grant e book number edition just dollar...\n",
       "...                                                 ...\n",
       "1391  want to be your own boss nbsp train now with s...\n",
       "1392  this is a multi part messag in mime format nex...\n",
       "1393  dear subscriber if i could show you a way to g...\n",
       "1394  mid summ custom appreci sale to express our ap...\n",
       "1395  attn sir madan strict confidential i am pleas ...\n",
       "\n",
       "[1396 rows x 1 columns]>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiser les donnes spam\n",
    "spam = pd.DataFrame(spam_mails_clean)\n",
    "spam.head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hYl6Ry2Uf2e7"
   },
   "source": [
    "2.  Construction du vocabulaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558060\n"
     ]
    }
   ],
   "source": [
    "# Joindre tous les mails nettoyés en une seule chaîne de caractères\n",
    "all_spam_mails = \" \".join(spam_mails_clean)\n",
    "# obtenir tous les mots de tous les emails spam\n",
    "all_words = all_spam_mails.split()  \n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4010965\n"
     ]
    }
   ],
   "source": [
    "print(len(all_spam_mails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_from_file():\n",
    "    with open(\"vocab.txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        vocab_list = content.split()\n",
    "    return vocab_list\n",
    "\n",
    "def build_and_save_vocab(all_words_mails, k):\n",
    "   # Compter le nombre d'occurrences de chaque mot\n",
    "    all_word_counts = np.unique(all_words_mails, return_counts=True)\n",
    "    # Trouver les indices des mots qui apparaissent k fois ou plus\n",
    "    frequent_word_indices = np.where(all_word_counts[1] >= k)[0]\n",
    "    # Extraire les mots fréquents et les enregistrer dans le fichier \"vocab.txt\"\n",
    "    frequent_words = all_word_counts[0][frequent_word_indices]\n",
    "    np.savetxt(\"vocab.txt\", frequent_words, fmt=\"%s\")\n",
    "    return frequent_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construire une liste de vocabulaire, nous ajoutons les mots qui se répètent au moins K fois\n",
    "k = 10\n",
    "vocab_list = build_and_save_vocab(all_words ,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexation_mail(all_mails_clean):\n",
    "    all_mails_index = []# Liste pour stocker les index des mails\n",
    "    for mail in all_mails_clean:\n",
    "        mail_index = [] # Liste pour stocker les index des mots dans un mail\n",
    "        for word in mail.split():\n",
    "            index_word = np.searchsorted(vocab_list, word)  # Recherche de l'index du mot dans le vocabulaire\n",
    "            if index_word < len(vocab_list) and vocab_list[index_word] == word :\n",
    "                   mail_index.append(index_word) # Ajout de l'index du mot à la liste des index du mail si le mot existe dans vocab_list\n",
    "        all_mails_index.append(mail_index)  # Ajout le mail indexe à la liste de tous les mails\n",
    "    return all_mails_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexer les mails\n",
    "spam_mails_index = indexation_mail(spam_mails_clean)\n",
    "hard_ham_mails_index = indexation_mail(hard_ham_mails_clean)\n",
    "easy_ham_mails_index = indexation_mail(easy_ham_mails_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spam = np.ones(len(spam_mails_index) , dtype=int)\n",
    "y_ham = np.zeros(len(hard_ham_mails_index) + len(easy_ham_mails_index) , dtype=int)\n",
    "# Concaténation des tableaux 'spam_mails_index', 'hard_ham_mails_index' et 'easy_ham_mails_index' dans 'all_mails_index'\n",
    "all_mails_index = np.concatenate((spam_mails_index , hard_ham_mails_index , easy_ham_mails_index) , axis=0)\n",
    "# Concaténation des tableaux 'y_spam' et 'y_ham' dans 'y'\n",
    "y = np.concatenate((y_spam , y_ham),axis=0)\n",
    "m = len(all_mails_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4197, 3740)\n",
      "(4197,)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((m, len(vocab_list)))\n",
    "# Parcours de chaque mail dans la liste des index de tous les mails\n",
    "for i in range(len(all_mails_index)):\n",
    "    mail_index = all_mails_index[i]\n",
    "    for index in mail_index:\n",
    "        X[i, index] = X[i, index] + 1  # Incrémentation du compteur de fréquence du mot correspondant dans le tableau X\n",
    "        \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des données X en les mettant à l'échelle\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_f = scaler.transform(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LEvfJ63Vf2e9"
   },
   "source": [
    "# Étape 2 : Implémentation des modèle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  1. Reseaux de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'apprentissage et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_f, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prsesion : 98.09523809523809\n"
     ]
    }
   ],
   "source": [
    "# Définition de la taille de l'entrée et de la sortie\n",
    "#  couche 1 : len mails  *  25\n",
    "#  couche 2  : 25 * 10\n",
    "#  couche 3 : 10 * 1\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(25, 10))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur de nouvelles données\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "precision = np.mean(y_test == y_pred) * 100\n",
    "\n",
    "print(\"Prsesion : \" + str(precision))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prsesion : 95.47619047619048\n"
     ]
    }
   ],
   "source": [
    "model= svm.SVC(kernel='linear',probability=True) \n",
    "# entrainement \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur de nouvelles données\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "precision = np.mean(y_test == y_pred) * 100\n",
    "\n",
    "print(\"Prsesion : \" + str(precision))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Arbre de desicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prsesion : 93.57142857142857\n"
     ]
    }
   ],
   "source": [
    "model= tree.DecisionTreeClassifier()\n",
    "# entrainement \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur de nouvelles données\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "precision = np.mean(y_test == y_pred) * 100\n",
    "\n",
    "print(\"Prsesion : \" + str(precision))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prsesion : 84.76190476190476\n"
     ]
    }
   ],
   "source": [
    "model= KNeighborsClassifier(n_neighbors=3)\n",
    "# entrainement \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur de nouvelles données\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "precision = np.mean(y_test == y_pred) * 100\n",
    "\n",
    "print(\"Prsesion : \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prsesion : 97.61904761904762\n"
     ]
    }
   ],
   "source": [
    "model= LogisticRegression(max_iter=1000)\n",
    "# entrainement \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur de nouvelles données\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "precision = np.mean(y_test == y_pred) * 100\n",
    "\n",
    "print(\"Prsesion : \" + str(precision))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats\n",
    "\n",
    "Le tableau suivant contient les résultats des precision calculés sur l'ensemble de test pour chaque algorithme en utilisant le threshold calculé avec la moyenne géométrique de la spécificité et la sensibilité :\n",
    "   \n",
    "| Modèle   | RNN         | SVM         | Arbre de desicion | KNN         |  Regression logistique   |\n",
    "|----------|-------------|-------------|-------------------|-------------|--------------------------|\n",
    "| Precision | 98.10%     | 97.97%      | 93.10%            | 91.55%      | 97.14%                   |\n",
    "\n",
    "En utilisant la normalisation nous n'obtenons pas d'améliorations significatives :\n",
    "| Modèle   | RNN         | SVM         | Arbre de desicion | KNN         |  Regression logistique   |\n",
    "|----------|-------------|-------------|-------------------|-------------|--------------------------|\n",
    "| Precision | 97.62%     | 95.48%      | 93.57%            | 84.76%      | 97.62%                   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "Les reseaux du neurones est le modèle le plus approprié pour notre classifieur SPAM."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP 2 Descente du gradient.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "71e3a6fbe6098334c1879ca135f58fe03869bd5aa00ac3d3b1c97e34672e0aa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
